<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>monk.gluon.finetune.level_8_layers_main API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>monk.gluon.finetune.level_8_layers_main</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math
from gluon.finetune.imports import *
from system.imports import *
from gluon.finetune.level_7_aux_main import prototype_aux


class prototype_layers(prototype_aux):
    &#39;&#39;&#39;
    Main class for all layers and activations
        - Layers and activations while appending to base network
        - Layers and activations while creating custom network

    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    &#39;&#39;&#39;

    def __init__(self, verbose=1):
        super().__init__(verbose=verbose);

    #####################################################################################################################################
    def append_linear(self, num_neurons=False, final_layer=False):
        &#39;&#39;&#39;
        Append dense (fully connected) layer to base network in transfer learning

        Args:
            num_neurons (int): Number of neurons in the dense layer
            final_layer (bool): If True, then number of neurons are directly set as number of classes in dataset for single label type classification

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            if(not num_neurons):
                num_neurons = self.system_dict[&#34;dataset&#34;][&#34;params&#34;][&#34;num_classes&#34;];
            self.system_dict = layer_linear(self.system_dict, num_neurons=num_neurons, final_layer=final_layer);
    #####################################################################################################################################


    #####################################################################################################################################
    def append_dropout(self, probability=0.5, final_layer=False):
        &#39;&#39;&#39;
        Append dropout layer to base network in transfer learning

        Args:
            probability (float): Droping probability of neurons in next layer
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = layer_dropout(self.system_dict, probability=probability, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_relu(self, final_layer=False):
        &#39;&#39;&#39;
        Append rectified linear unit activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_relu(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################




    #####################################################################################################################################
    def append_sigmoid(self, final_layer=False):
        &#39;&#39;&#39;
        Append sigmoid activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_sigmoid(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_tanh(self, final_layer=False):
        &#39;&#39;&#39;
        Append tanh activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_tanh(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_softplus(self, beta=1, threshold=20, final_layer=False):
        &#39;&#39;&#39;
        Append softplus activation to base network in transfer learning

        Args:
            beta (int): Multiplicative factor
            threshold (int): softplus (thresholded relu) limit 
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_softplus(self.system_dict, beta=1, threshold=20, final_layer=False);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_softsign(self, final_layer=False):
        &#39;&#39;&#39;
        Append softsign activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_softsign(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_leakyrelu(self, negative_slope=0.01, final_layer=False):
        &#39;&#39;&#39;
        Append Leaky - ReLU activation to base network in transfer learning

        Args:
            negative_slope (float): Multiplicatve factor towards negative spectrum of real numbers.
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_leakyrelu(self.system_dict, negative_slope=negative_slope, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_prelu(self, num_parameters=1, init=0.25, final_layer=False):
        &#39;&#39;&#39;
        Append Learnable parameerized rectified linear unit activation to base network in transfer learning

        Args:
            init (float): Initialization value for multiplicatve factor towards negative spectrum of real numbers.
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_prelu(self.system_dict, num_parameters=num_parameters, init=init, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_elu(self, alpha=1.0, final_layer=False):
        &#39;&#39;&#39;
        Append exponential linear unit activation to base network in transfer learning

        Args:
            alpha (float): Multiplicatve factor.
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_elu(self.system_dict, alpha=alpha, final_layer=final_layer); 
    #####################################################################################################################################



    #####################################################################################################################################
    def append_selu(self, final_layer=False):
        &#39;&#39;&#39;
        Append scaled exponential linear unit activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_selu(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_swish(self, beta=1.0, final_layer=False):
        &#39;&#39;&#39;
        Append swish activation to base network in transfer learning

        Args:
            beta (bool): Multiplicative factor
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_swish(self.system_dict, beta=beta, final_layer=final_layer);
    #####################################################################################################################################





    #####################################################################################################################################
    def convolution1d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def convolution2d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def convolution(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def convolution3d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def transposed_convolution1d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def transposed_convolution(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def transposed_convolution2d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def transposed_convolution3d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def max_pooling1d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################





    #####################################################################################################################################
    def max_pooling2d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def max_pooling(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def max_pooling3d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def average_pooling1d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            include_padding_in_calculation (bool): If True, padding will be considered.
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def average_pooling2d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            include_padding_in_calculation (bool): If True, padding will be considered.
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def average_pooling(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            include_padding_in_calculation (bool): If True, padding will be considered.
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def average_pooling3d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            include_padding_in_calculation (bool): If True, padding will be considered.
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;  
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_max_pooling1d(self, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_max_pooling2d(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp; 
    #####################################################################################################################################




    #####################################################################################################################################
    def global_max_pooling(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_max_pooling3d(self, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling1d(self, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling2d(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling3d(self, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################


    #####################################################################################################################################
    def fully_connected(self, units=512, use_bias=True, flatten=True, uid=None):
        &#39;&#39;&#39;
        Append fully-connected (dense) layer to custom network

        Args:
            units (int): Number of neurons in the layer
            use_bias (bool): If True, learnable bias is added
            flatten (bool): Fixed to True
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;fully_connected&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;units&#34;] = units; 
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias; 
        tmp[&#34;params&#34;][&#34;flatten&#34;] = flatten; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def dropout(self, drop_probability=0.2, axes=(), uid=None):
        &#39;&#39;&#39;
        Append dropout layer to custom network

        Args:
            drop_probability (float): Probability for not considering neurons in the output
            axes (tuple): Channel axis to implement dropout over
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;dropout&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;drop_probability&#34;] = drop_probability;
        tmp[&#34;params&#34;][&#34;axes&#34;] = axes;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def flatten(self, uid=None):
        &#39;&#39;&#39;
        Append flatten layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;flatten&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def identity(self, uid=None):
        &#39;&#39;&#39;
        Append identity layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;identity&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def add(self, uid=None):
        &#39;&#39;&#39;
        Append elementwise addition layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;add&#34;;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def concatenate(self, uid=None):
        &#39;&#39;&#39;
        Append concatenation layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;concatenate&#34;;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def batch_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
        activate_scale_shift_operation=False, uid=None):
        &#39;&#39;&#39;
        Append batch normalization layer to custom network

        Args:
            moving_average_momentum (float): Normalization momentum value
            epsilon (float): Value to avoid division by zero
            use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
            activate_scale_shift_operation (bool): Fixed status - False
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;batch_normalization&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;moving_average_momentum&#34;] = moving_average_momentum;
        tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
        tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
        tmp[&#34;params&#34;][&#34;activate_scale_shift_operation&#34;] = activate_scale_shift_operation;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    @TraceFunction(trace_args=True, trace_rv=True)
    def instance_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
        uid=None):
        &#39;&#39;&#39;
        Append instace normalization layer to custom network

        Args:
            moving_average_momentum (float): Normalization momentum value
            epsilon (float): Value to avoid division by zero
            use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;instance_normalization&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
        tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def layer_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
        uid=None):
        &#39;&#39;&#39;
        Append layer normalization layer to custom network

        Args:
            moving_average_momentum (float): Normalization momentum value
            epsilon (float): Value to avoid division by zero
            use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;layer_normalization&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
        tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def relu(self, uid=None):
        &#39;&#39;&#39;
        Append rectified linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;relu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def sigmoid(self, uid=None):
        &#39;&#39;&#39;
        Append sigmoid activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;sigmoid&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def tanh(self, uid=None):
        &#39;&#39;&#39;
        Append tanh activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;tanh&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def softplus(self, beta=1, threshold=20, uid=None): 
        &#39;&#39;&#39;
        Append softplus activation to custom network

        Args:
            beta (int): Multiplicative factor
            threshold (int): softplus (thresholded relu) limit 
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;softplus&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;beta&#34;] = beta;
        tmp[&#34;params&#34;][&#34;threshold&#34;] = threshold;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def softsign(self, uid=None): 
        &#39;&#39;&#39;
        Append softsign activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;softsign&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def elu(self, alpha=1.0, uid=None): 
        &#39;&#39;&#39;
        Append exponential linear unit activation to custom network

        Args:
            alpha (float): Multiplicative factor
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;elu&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;alpha&#34;] = alpha;
        return tmp;
    #####################################################################################################################################


    #####################################################################################################################################
    def gelu(self, uid=None): 
        &#39;&#39;&#39;
        Append gated exponential linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;gelu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################


    #####################################################################################################################################
    def leaky_relu(self, alpha=0.01, uid=None): 
        &#39;&#39;&#39;
        Append leaky relu activation to custom network

        Args:
            alpha (float): Multiplicatve factor towards negative spectrum of real numbers.
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;leaky_relu&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;alpha&#34;] = alpha;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def prelu(self, uid=None): 
        &#39;&#39;&#39;
        Append paramemeterized rectified linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;prelu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def selu(self, uid=None): 
        &#39;&#39;&#39;
        Append scaled exponential linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;selu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def swish(self, beta=1.0, uid=None): 
        &#39;&#39;&#39;
        Append swish activation to custom network

        Args:
            beta (float): Multiplicative factor
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;swish&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;beta&#34;] = beta;
        return tmp;
    #####################################################################################################################################


    
    






    #####################################################################################################################################
    def resnet_v1_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V1 Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int, tuple): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
    
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=1));
        branch_1.append(self.batch_normalization());
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
            branch_2.append(self.batch_normalization());
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        network.append(self.relu());
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def resnet_v2_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V2 Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int, tuple): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        network.append(self.batch_normalization());
        network.append(self.relu());
        
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=1));
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def resnet_v1_bottleneck_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V1 Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int, tuple): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
    
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=1, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=3, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
            branch_2.append(self.batch_normalization());
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        network.append(self.relu())
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def resnet_v2_bottleneck_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V2 Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        network.append(self.batch_normalization());
        network.append(self.relu());
        
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def resnext_block(self, output_channels=256, cardinality=8, bottleneck_width=4, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnext Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            cardinality (int): cardinality dimensions for complex transformations
            bottleneck_width (int): Bottleneck dimensions for reducing number of features
            stride (int): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
    
        channels = output_channels//4;
        D = int(math.floor(channels * (bottleneck_width / 64)))
        group_width = cardinality * D
        
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=group_width, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=group_width, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
            branch_2.append(self.batch_normalization());
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        network.append(self.relu());
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def mobilenet_v2_linear_bottleneck_block(self, output_channels=32, bottleneck_width=4, stride=1):
        &#39;&#39;&#39;
        Append Mobilenet V2 Linear Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int): kernel movement stride  
            bottleneck_width (int): Bottleneck dimensions for reducing number of features

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        if(bottleneck_width != 1):
            branch_1.append(self.convolution(output_channels=output_channels*bottleneck_width, 
                                            kernel_size=1, stride=1));
        
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels*bottleneck_width,
                                        kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        
        branch_2 = [];
        branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def separable_convolution_block(self, input_channels=16, output_channels=32, kernel_size=3, stride=1, padding=1):
        &#39;&#39;&#39;
        Append Separable convolution Block to custom network

        Args:
            input_channels (int): Number of input features for this block
            output_channels (int): Number of output features for this block
            kernel_size (int): Kernel matrix shape for all layers in this block
            stride (int): kernel movement stride  
            padding (int, tuple): external zero padding on input

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        network.append(self.convolution(output_channels=input_channels, kernel_size=kernel_size, 
                                       stride=stride, padding=padding, groups=input_channels));
        network.append(self.convolution(output_channels=output_channels, kernel_size=1, 
                                       stride=1));

        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def mobilenet_v2_inverted_linear_bottleneck_block(self, output_channels=32, bottleneck_width=4, stride=1):
        &#39;&#39;&#39;
        Append Mobilenet V2 Inverted Linear Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int): kernel movement stride  
            bottleneck_width (int): Bottleneck dimensions for reducing number of features

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        if(bottleneck_width != 1):
            branch_1.append(self.convolution(output_channels=output_channels//bottleneck_width, 
                                            kernel_size=1, stride=1));
        
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        sep_conv = self.separable_convolution_block(input_channels=output_channels//bottleneck_width,
                                                        output_channels=output_channels//bottleneck_width,
                                                        kernel_size=3, stride=stride);
        branch_1.append(sep_conv);   
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        
        branch_2 = [];
        branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################




    #####################################################################################################################################    
    def squeezenet_fire_block(self, squeeze_channels=16, expand_channels_1x1=32, expand_channels_3x3=64):
        &#39;&#39;&#39;
        Append Squeezenet Fire Block to custom network

        Args:
            squeeze_channels (int): Number of output features for this block
            expand_channels_1x1 (int): Number of convolution_1x1 features for this block
            expand_channels_3x3 (int): Number of convolution_3x3 features for this block
            bottleneck_width (int): Bottleneck dimensions for reducing number of features

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        network.append(self.convolution(output_channels=squeeze_channels, kernel_size=1, stride=1));
        network.append(self.relu());
        
        subnetwork = [];
        branch_1 = [];    
        branch_2 = [];
        
        branch_1.append(self.convolution(output_channels=expand_channels_1x1, kernel_size=1, stride=1));
        branch_1.append(self.relu());
        
        branch_2.append(self.convolution(output_channels=expand_channels_3x3, kernel_size=3, stride=1));
        branch_2.append(self.relu());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def densenet_block(self, bottleneck_size=4, growth_rate=16, dropout=0.2):
        &#39;&#39;&#39;
        Append Densenet Block to custom network

        Args:
            bottleneck_size (int): Bottleneck dimensions for reducing number of features
            growth_rate (int): Expansion rate for convolution layers for this block
            dropout (float): Prbability for dropout layer post convolution

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=bottleneck_size*growth_rate, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=growth_rate, kernel_size=3, stride=1));
        branch_1.append(self.dropout(drop_probability=dropout));
        
        branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def conv_bn_relu_block(self, output_channels=64, kernel_size=1, stride=1, padding=None):
        &#39;&#39;&#39;
        Append Conv-&gt;batch_norm-&gt;relu Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            kernel_size (int): Kernel matrix shape for all layers in this block
            stride (int): kernel movement stride  
            padding (int, tuple): external zero padding on input

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        if(padding):
            network.append(self.convolution(output_channels=output_channels, 
                                           kernel_size=kernel_size, 
                                           stride=stride,
                                           padding=padding));
        else:
            network.append(self.convolution(output_channels=output_channels, 
                                           kernel_size=kernel_size, 
                                           stride=stride));
        network.append(self.batch_normalization());
        network.append(self.relu());
        
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def inception_a_block(self, pooling_branch_channels=32, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-A Block to custom network

        Args:
            pooling_branch_channels (int): Number of features for conv layers in pooling branch
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1))
        
        branch_2.append(self.conv_bn_relu_block(output_channels=48, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=64, kernel_size=5));
           
        branch_3.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1));
        branch_3.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        branch_3.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        
        if(pool_type==&#34;avg&#34;):
            branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        branch_4.append(self.conv_bn_relu_block(output_channels=pooling_branch_channels, kernel_size=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(branch_4);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################



    #####################################################################################################################################
    def inception_b_block(self, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-B Block to custom network

        Args:
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=384, kernel_size=3))
              
        branch_2.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        branch_2.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        
        if(pool_type==&#34;avg&#34;):
            branch_3.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_3.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);

        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def inception_c_block(self, channels_7x7=3, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-C Block to custom network

        Args:
            channels_7x7 (int): Number of features for conv layers in channels_7x7 branch
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1))
        
        
        branch_2.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
           
            
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=1));
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(7, 1), padding=(3, 0)));
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
        branch_3.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
        
        if(pool_type==&#34;avg&#34;):
            branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        branch_4.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(branch_4);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################



    #####################################################################################################################################
    def inception_d_block(self, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-D Block to custom network

        Args:
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1))
        branch_1.append(self.conv_bn_relu_block(output_channels=320, kernel_size=3, stride=2))
        
        
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(1, 7), padding=(0, 3)));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=3, stride=2));

        
        if(pool_type==&#34;avg&#34;):
            branch_3.append(self.average_pooling(kernel_size=3, stride=2));
        else:
            branch_3.append(self.max_pooling(kernel_size=3, stride=2));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def subbranch_block(self):  
        &#39;&#39;&#39;
        Append sub-branch Block to custom network

        Args:
            None
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;  
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_1.append(self.conv_bn_relu_block(output_channels=384, kernel_size=(1, 3), padding=(0, 1)));
        branch_2.append(self.conv_bn_relu_block(output_channels=384, kernel_size=(3, 1), padding=(1, 0)));
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.concatenate());
        return subnetwork;
    #####################################################################################################################################




    #####################################################################################################################################
    def inception_e_block(self, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-C Block to custom network

        Args:
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=320, kernel_size=1))
        
        branch_2.append(self.conv_bn_relu_block(output_channels=384, kernel_size=1));
        branch_2.append(self.subbranch_block());
        
        
        branch_3.append(self.conv_bn_relu_block(output_channels=448, kernel_size=1));
        branch_3.append(self.conv_bn_relu_block(output_channels=384, kernel_size=3, padding=1));
        branch_3.append(self.subbranch_block());
        

        
        if(pool_type==&#34;avg&#34;):
            branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        branch_4.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(branch_4);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers"><code class="flex name class">
<span>class <span class="ident">prototype_layers</span></span>
<span>(</span><span>verbose=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Main class for all layers and activations
- Layers and activations while appending to base network
- Layers and activations while creating custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code></dt>
<dd>Set verbosity levels
0 - Print Nothing
1 - Print desired details</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class prototype_layers(prototype_aux):
    &#39;&#39;&#39;
    Main class for all layers and activations
        - Layers and activations while appending to base network
        - Layers and activations while creating custom network

    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    &#39;&#39;&#39;

    def __init__(self, verbose=1):
        super().__init__(verbose=verbose);

    #####################################################################################################################################
    def append_linear(self, num_neurons=False, final_layer=False):
        &#39;&#39;&#39;
        Append dense (fully connected) layer to base network in transfer learning

        Args:
            num_neurons (int): Number of neurons in the dense layer
            final_layer (bool): If True, then number of neurons are directly set as number of classes in dataset for single label type classification

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            if(not num_neurons):
                num_neurons = self.system_dict[&#34;dataset&#34;][&#34;params&#34;][&#34;num_classes&#34;];
            self.system_dict = layer_linear(self.system_dict, num_neurons=num_neurons, final_layer=final_layer);
    #####################################################################################################################################


    #####################################################################################################################################
    def append_dropout(self, probability=0.5, final_layer=False):
        &#39;&#39;&#39;
        Append dropout layer to base network in transfer learning

        Args:
            probability (float): Droping probability of neurons in next layer
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = layer_dropout(self.system_dict, probability=probability, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_relu(self, final_layer=False):
        &#39;&#39;&#39;
        Append rectified linear unit activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_relu(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################




    #####################################################################################################################################
    def append_sigmoid(self, final_layer=False):
        &#39;&#39;&#39;
        Append sigmoid activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_sigmoid(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_tanh(self, final_layer=False):
        &#39;&#39;&#39;
        Append tanh activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_tanh(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_softplus(self, beta=1, threshold=20, final_layer=False):
        &#39;&#39;&#39;
        Append softplus activation to base network in transfer learning

        Args:
            beta (int): Multiplicative factor
            threshold (int): softplus (thresholded relu) limit 
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_softplus(self.system_dict, beta=1, threshold=20, final_layer=False);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_softsign(self, final_layer=False):
        &#39;&#39;&#39;
        Append softsign activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_softsign(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_leakyrelu(self, negative_slope=0.01, final_layer=False):
        &#39;&#39;&#39;
        Append Leaky - ReLU activation to base network in transfer learning

        Args:
            negative_slope (float): Multiplicatve factor towards negative spectrum of real numbers.
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_leakyrelu(self.system_dict, negative_slope=negative_slope, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_prelu(self, num_parameters=1, init=0.25, final_layer=False):
        &#39;&#39;&#39;
        Append Learnable parameerized rectified linear unit activation to base network in transfer learning

        Args:
            init (float): Initialization value for multiplicatve factor towards negative spectrum of real numbers.
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_prelu(self.system_dict, num_parameters=num_parameters, init=init, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_elu(self, alpha=1.0, final_layer=False):
        &#39;&#39;&#39;
        Append exponential linear unit activation to base network in transfer learning

        Args:
            alpha (float): Multiplicatve factor.
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_elu(self.system_dict, alpha=alpha, final_layer=final_layer); 
    #####################################################################################################################################



    #####################################################################################################################################
    def append_selu(self, final_layer=False):
        &#39;&#39;&#39;
        Append scaled exponential linear unit activation to base network in transfer learning

        Args:
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_selu(self.system_dict, final_layer=final_layer);
    #####################################################################################################################################



    #####################################################################################################################################
    def append_swish(self, beta=1.0, final_layer=False):
        &#39;&#39;&#39;
        Append swish activation to base network in transfer learning

        Args:
            beta (bool): Multiplicative factor
            final_layer (bool): Indicator that this layer marks the end of network.

        Returns:
            None
        &#39;&#39;&#39;
        if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
            msg = &#34;Cannot append more layers.\n&#34;;
            msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
            raise ConstraintError(msg);
        else:
            self.system_dict = activation_swish(self.system_dict, beta=beta, final_layer=final_layer);
    #####################################################################################################################################





    #####################################################################################################################################
    def convolution1d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def convolution2d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def convolution(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def convolution3d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        groups=1, dilation=1, use_bias=True, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;convolution3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def transposed_convolution1d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def transposed_convolution(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def transposed_convolution2d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def transposed_convolution3d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
        output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-transposed-convolution to custom network

        Args:
            output_channels (int): Number of output features for this layer
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            output_padding (int): Additional padding applied to output
            groups (int): Number of groups for grouped convolution
            dilation (int): Factor for dilated convolution
            use_bias (bool): If True, learnable bias is added
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp={};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;transposed_convolution3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
        tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def max_pooling1d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################





    #####################################################################################################################################
    def max_pooling2d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def max_pooling(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def max_pooling3d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-max-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;max_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################




    #####################################################################################################################################
    def average_pooling1d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            include_padding_in_calculation (bool): If True, padding will be considered.
            ceil_mode (bool): If True, apply ceil math operation post pooling
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def average_pooling2d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            include_padding_in_calculation (bool): If True, padding will be considered.
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def average_pooling(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            include_padding_in_calculation (bool): If True, padding will be considered.
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def average_pooling3d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
        return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
        layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-average-pooling to custom network

        Args:
            kernel_size (int, tuple): kernel matrix size 
            stride (int, tuple): kernel movement stride  
            padding (int, tuple, str): Zero padding applied to input
                                        1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                        2) integer or tuple value: Manually add padding 
            dilation (int): Factor for dilated pooling
            return_indices (bool): Fixed value set as False
            ceil_mode (bool): If True, apply ceil math operation post pooling
            include_padding_in_calculation (bool): If True, padding will be considered.
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;average_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
        tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
        tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
        tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
        tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
        tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
        tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;  
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_max_pooling1d(self, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_max_pooling2d(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp; 
    #####################################################################################################################################




    #####################################################################################################################################
    def global_max_pooling(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_max_pooling3d(self, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-global-max-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_max_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling1d(self, layout=&#39;NCW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 1d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCW&#39; - order
                            2) &#39;NWC&#39; - order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling1d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling2d(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling(self, layout=&#39;NCHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 2d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCHW&#39; - Order
                            2) &#39;NHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling2d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def global_average_pooling3d(self, layout=&#39;NCDHW&#39;, uid=None):
        &#39;&#39;&#39;
        Append 3d-global-average-pooling to custom network

        Args:
            layout (str): Either of these values (order)
                            1) &#39;NCDHW&#39; - Order
                            2) &#39;NDHWC&#39; - Order
                            - N: Number of elements in batches
                            - C: Number of channels
                            - D: Depth of features in layers
                            - H: Height of features in layers
                            - W: Number of features in layers
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;global_average_pooling3d&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
        return tmp;
    #####################################################################################################################################


    #####################################################################################################################################
    def fully_connected(self, units=512, use_bias=True, flatten=True, uid=None):
        &#39;&#39;&#39;
        Append fully-connected (dense) layer to custom network

        Args:
            units (int): Number of neurons in the layer
            use_bias (bool): If True, learnable bias is added
            flatten (bool): Fixed to True
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;fully_connected&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;units&#34;] = units; 
        tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias; 
        tmp[&#34;params&#34;][&#34;flatten&#34;] = flatten; 
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def dropout(self, drop_probability=0.2, axes=(), uid=None):
        &#39;&#39;&#39;
        Append dropout layer to custom network

        Args:
            drop_probability (float): Probability for not considering neurons in the output
            axes (tuple): Channel axis to implement dropout over
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;dropout&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;drop_probability&#34;] = drop_probability;
        tmp[&#34;params&#34;][&#34;axes&#34;] = axes;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def flatten(self, uid=None):
        &#39;&#39;&#39;
        Append flatten layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;flatten&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def identity(self, uid=None):
        &#39;&#39;&#39;
        Append identity layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;identity&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def add(self, uid=None):
        &#39;&#39;&#39;
        Append elementwise addition layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;add&#34;;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def concatenate(self, uid=None):
        &#39;&#39;&#39;
        Append concatenation layer to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;concatenate&#34;;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def batch_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
        activate_scale_shift_operation=False, uid=None):
        &#39;&#39;&#39;
        Append batch normalization layer to custom network

        Args:
            moving_average_momentum (float): Normalization momentum value
            epsilon (float): Value to avoid division by zero
            use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
            activate_scale_shift_operation (bool): Fixed status - False
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;batch_normalization&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;moving_average_momentum&#34;] = moving_average_momentum;
        tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
        tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
        tmp[&#34;params&#34;][&#34;activate_scale_shift_operation&#34;] = activate_scale_shift_operation;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    @TraceFunction(trace_args=True, trace_rv=True)
    def instance_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
        uid=None):
        &#39;&#39;&#39;
        Append instace normalization layer to custom network

        Args:
            moving_average_momentum (float): Normalization momentum value
            epsilon (float): Value to avoid division by zero
            use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;instance_normalization&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
        tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def layer_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
        uid=None):
        &#39;&#39;&#39;
        Append layer normalization layer to custom network

        Args:
            moving_average_momentum (float): Normalization momentum value
            epsilon (float): Value to avoid division by zero
            use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;layer_normalization&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
        tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def relu(self, uid=None):
        &#39;&#39;&#39;
        Append rectified linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;relu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def sigmoid(self, uid=None):
        &#39;&#39;&#39;
        Append sigmoid activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;sigmoid&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def tanh(self, uid=None):
        &#39;&#39;&#39;
        Append tanh activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;tanh&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def softplus(self, beta=1, threshold=20, uid=None): 
        &#39;&#39;&#39;
        Append softplus activation to custom network

        Args:
            beta (int): Multiplicative factor
            threshold (int): softplus (thresholded relu) limit 
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;softplus&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;beta&#34;] = beta;
        tmp[&#34;params&#34;][&#34;threshold&#34;] = threshold;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def softsign(self, uid=None): 
        &#39;&#39;&#39;
        Append softsign activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;softsign&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def elu(self, alpha=1.0, uid=None): 
        &#39;&#39;&#39;
        Append exponential linear unit activation to custom network

        Args:
            alpha (float): Multiplicative factor
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;elu&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;alpha&#34;] = alpha;
        return tmp;
    #####################################################################################################################################


    #####################################################################################################################################
    def gelu(self, uid=None): 
        &#39;&#39;&#39;
        Append gated exponential linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;gelu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################


    #####################################################################################################################################
    def leaky_relu(self, alpha=0.01, uid=None): 
        &#39;&#39;&#39;
        Append leaky relu activation to custom network

        Args:
            alpha (float): Multiplicatve factor towards negative spectrum of real numbers.
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;leaky_relu&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;alpha&#34;] = alpha;
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def prelu(self, uid=None): 
        &#39;&#39;&#39;
        Append paramemeterized rectified linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;prelu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def selu(self, uid=None): 
        &#39;&#39;&#39;
        Append scaled exponential linear unit activation to custom network

        Args:
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;selu&#34;;
        tmp[&#34;params&#34;] = {};
        return tmp;
    #####################################################################################################################################



    #####################################################################################################################################
    def swish(self, beta=1.0, uid=None): 
        &#39;&#39;&#39;
        Append swish activation to custom network

        Args:
            beta (float): Multiplicative factor
            uid (str): Unique name for layer, if not mentioned then dynamically assigned

        Returns:
            dict: Containing all the parameters set as per function arguments
        &#39;&#39;&#39;
        tmp = {};
        tmp[&#34;uid&#34;] = uid;
        tmp[&#34;name&#34;] = &#34;swish&#34;;
        tmp[&#34;params&#34;] = {};
        tmp[&#34;params&#34;][&#34;beta&#34;] = beta;
        return tmp;
    #####################################################################################################################################


    
    






    #####################################################################################################################################
    def resnet_v1_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V1 Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int, tuple): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
    
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=1));
        branch_1.append(self.batch_normalization());
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
            branch_2.append(self.batch_normalization());
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        network.append(self.relu());
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def resnet_v2_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V2 Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int, tuple): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        network.append(self.batch_normalization());
        network.append(self.relu());
        
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=1));
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def resnet_v1_bottleneck_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V1 Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int, tuple): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
    
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=1, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=3, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
            branch_2.append(self.batch_normalization());
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        network.append(self.relu())
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def resnet_v2_bottleneck_block(self, output_channels=16, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnet V2 Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        network.append(self.batch_normalization());
        network.append(self.relu());
        
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def resnext_block(self, output_channels=256, cardinality=8, bottleneck_width=4, stride=1, downsample=True):
        &#39;&#39;&#39;
        Append Resnext Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            cardinality (int): cardinality dimensions for complex transformations
            bottleneck_width (int): Bottleneck dimensions for reducing number of features
            stride (int): kernel movement stride  
            downsample (bool): If False, residual branch is a shortcut,
                                Else, residual branch has non-identity layers

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
    
        channels = output_channels//4;
        D = int(math.floor(channels * (bottleneck_width / 64)))
        group_width = cardinality * D
        
        subnetwork = [];
        branch_1 = [];
        branch_1.append(self.convolution(output_channels=group_width, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=group_width, kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        
        
        branch_2 = [];
        if(downsample):
            branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
            branch_2.append(self.batch_normalization());
        else:
            branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork)
        network.append(self.relu());
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def mobilenet_v2_linear_bottleneck_block(self, output_channels=32, bottleneck_width=4, stride=1):
        &#39;&#39;&#39;
        Append Mobilenet V2 Linear Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int): kernel movement stride  
            bottleneck_width (int): Bottleneck dimensions for reducing number of features

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        if(bottleneck_width != 1):
            branch_1.append(self.convolution(output_channels=output_channels*bottleneck_width, 
                                            kernel_size=1, stride=1));
        
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels*bottleneck_width,
                                        kernel_size=3, stride=stride));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        
        branch_2 = [];
        branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def separable_convolution_block(self, input_channels=16, output_channels=32, kernel_size=3, stride=1, padding=1):
        &#39;&#39;&#39;
        Append Separable convolution Block to custom network

        Args:
            input_channels (int): Number of input features for this block
            output_channels (int): Number of output features for this block
            kernel_size (int): Kernel matrix shape for all layers in this block
            stride (int): kernel movement stride  
            padding (int, tuple): external zero padding on input

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        network.append(self.convolution(output_channels=input_channels, kernel_size=kernel_size, 
                                       stride=stride, padding=padding, groups=input_channels));
        network.append(self.convolution(output_channels=output_channels, kernel_size=1, 
                                       stride=1));

        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def mobilenet_v2_inverted_linear_bottleneck_block(self, output_channels=32, bottleneck_width=4, stride=1):
        &#39;&#39;&#39;
        Append Mobilenet V2 Inverted Linear Bottleneck Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            stride (int): kernel movement stride  
            bottleneck_width (int): Bottleneck dimensions for reducing number of features

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        if(bottleneck_width != 1):
            branch_1.append(self.convolution(output_channels=output_channels//bottleneck_width, 
                                            kernel_size=1, stride=1));
        
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        sep_conv = self.separable_convolution_block(input_channels=output_channels//bottleneck_width,
                                                        output_channels=output_channels//bottleneck_width,
                                                        kernel_size=3, stride=stride);
        branch_1.append(sep_conv);   
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        
        
        branch_2 = [];
        branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.add());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################




    #####################################################################################################################################    
    def squeezenet_fire_block(self, squeeze_channels=16, expand_channels_1x1=32, expand_channels_3x3=64):
        &#39;&#39;&#39;
        Append Squeezenet Fire Block to custom network

        Args:
            squeeze_channels (int): Number of output features for this block
            expand_channels_1x1 (int): Number of convolution_1x1 features for this block
            expand_channels_3x3 (int): Number of convolution_3x3 features for this block
            bottleneck_width (int): Bottleneck dimensions for reducing number of features

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        network.append(self.convolution(output_channels=squeeze_channels, kernel_size=1, stride=1));
        network.append(self.relu());
        
        subnetwork = [];
        branch_1 = [];    
        branch_2 = [];
        
        branch_1.append(self.convolution(output_channels=expand_channels_1x1, kernel_size=1, stride=1));
        branch_1.append(self.relu());
        
        branch_2.append(self.convolution(output_channels=expand_channels_3x3, kernel_size=3, stride=1));
        branch_2.append(self.relu());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def densenet_block(self, bottleneck_size=4, growth_rate=16, dropout=0.2):
        &#39;&#39;&#39;
        Append Densenet Block to custom network

        Args:
            bottleneck_size (int): Bottleneck dimensions for reducing number of features
            growth_rate (int): Expansion rate for convolution layers for this block
            dropout (float): Prbability for dropout layer post convolution

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=bottleneck_size*growth_rate, kernel_size=1, stride=1));
        branch_1.append(self.batch_normalization());
        branch_1.append(self.relu());
        branch_1.append(self.convolution(output_channels=growth_rate, kernel_size=3, stride=1));
        branch_1.append(self.dropout(drop_probability=dropout));
        
        branch_2.append(self.identity());
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def conv_bn_relu_block(self, output_channels=64, kernel_size=1, stride=1, padding=None):
        &#39;&#39;&#39;
        Append Conv-&gt;batch_norm-&gt;relu Block to custom network

        Args:
            output_channels (int): Number of output features for this block
            kernel_size (int): Kernel matrix shape for all layers in this block
            stride (int): kernel movement stride  
            padding (int, tuple): external zero padding on input

        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        if(padding):
            network.append(self.convolution(output_channels=output_channels, 
                                           kernel_size=kernel_size, 
                                           stride=stride,
                                           padding=padding));
        else:
            network.append(self.convolution(output_channels=output_channels, 
                                           kernel_size=kernel_size, 
                                           stride=stride));
        network.append(self.batch_normalization());
        network.append(self.relu());
        
        return network;
    #####################################################################################################################################





    #####################################################################################################################################
    def inception_a_block(self, pooling_branch_channels=32, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-A Block to custom network

        Args:
            pooling_branch_channels (int): Number of features for conv layers in pooling branch
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1))
        
        branch_2.append(self.conv_bn_relu_block(output_channels=48, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=64, kernel_size=5));
           
        branch_3.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1));
        branch_3.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        branch_3.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        
        if(pool_type==&#34;avg&#34;):
            branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        branch_4.append(self.conv_bn_relu_block(output_channels=pooling_branch_channels, kernel_size=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(branch_4);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################



    #####################################################################################################################################
    def inception_b_block(self, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-B Block to custom network

        Args:
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=384, kernel_size=3))
              
        branch_2.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        branch_2.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
        
        if(pool_type==&#34;avg&#34;):
            branch_3.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_3.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);

        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def inception_c_block(self, channels_7x7=3, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-C Block to custom network

        Args:
            channels_7x7 (int): Number of features for conv layers in channels_7x7 branch
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1))
        
        
        branch_2.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
           
            
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=1));
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(7, 1), padding=(3, 0)));
        branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
        branch_3.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
        
        if(pool_type==&#34;avg&#34;):
            branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        branch_4.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(branch_4);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################



    #####################################################################################################################################
    def inception_d_block(self, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-D Block to custom network

        Args:
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1))
        branch_1.append(self.conv_bn_relu_block(output_channels=320, kernel_size=3, stride=2))
        
        
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(1, 7), padding=(0, 3)));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
        branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=3, stride=2));

        
        if(pool_type==&#34;avg&#34;):
            branch_3.append(self.average_pooling(kernel_size=3, stride=2));
        else:
            branch_3.append(self.max_pooling(kernel_size=3, stride=2));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;
    #####################################################################################################################################




    #####################################################################################################################################
    def subbranch_block(self):  
        &#39;&#39;&#39;
        Append sub-branch Block to custom network

        Args:
            None
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;  
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_1.append(self.conv_bn_relu_block(output_channels=384, kernel_size=(1, 3), padding=(0, 1)));
        branch_2.append(self.conv_bn_relu_block(output_channels=384, kernel_size=(3, 1), padding=(1, 0)));
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(self.concatenate());
        return subnetwork;
    #####################################################################################################################################




    #####################################################################################################################################
    def inception_e_block(self, pool_type=&#34;avg&#34;):
        &#39;&#39;&#39;
        Append Inception-C Block to custom network

        Args:
            pool_type (str): Either of these types
                                - &#34;avg&#34; - Average pooling
                                - &#34;max&#34; - Max pooling
            
        Returns:
            list: Containing all the layer dictionaries arranged as per function arguments
        &#39;&#39;&#39;
        network = [];
        
        subnetwork = [];
        branch_1 = [];
        branch_2 = [];
        branch_3 = [];
        branch_4 = [];
        
        branch_1.append(self.conv_bn_relu_block(output_channels=320, kernel_size=1))
        
        branch_2.append(self.conv_bn_relu_block(output_channels=384, kernel_size=1));
        branch_2.append(self.subbranch_block());
        
        
        branch_3.append(self.conv_bn_relu_block(output_channels=448, kernel_size=1));
        branch_3.append(self.conv_bn_relu_block(output_channels=384, kernel_size=3, padding=1));
        branch_3.append(self.subbranch_block());
        

        
        if(pool_type==&#34;avg&#34;):
            branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
        else:
            branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
        branch_4.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
        
        subnetwork.append(branch_1);
        subnetwork.append(branch_2);
        subnetwork.append(branch_3);
        subnetwork.append(branch_4);
        subnetwork.append(self.concatenate());
        
        network.append(subnetwork);
        
        return network;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gluon.finetune.level_7_aux_main.prototype_aux</li>
<li>gluon.finetune.level_6_params_main.prototype_params</li>
<li>gluon.finetune.level_5_state_base.finetune_state</li>
<li>gluon.finetune.level_4_evaluation_base.finetune_evaluation</li>
<li>gluon.finetune.level_3_training_base.finetune_training</li>
<li>gluon.finetune.level_2_model_base.finetune_model</li>
<li>gluon.finetune.level_1_dataset_base.finetune_dataset</li>
<li>system.base_class.system</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.instance_normalization"><code class="name">var <span class="ident">instance_normalization</span></code></dt>
<dd>
<section class="desc"><p>partial(func, <em>args, </em>*keywords) - new function with partial application
of the given arguments and keywords.</p></section>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append elementwise addition layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, uid=None):
    &#39;&#39;&#39;
    Append elementwise addition layer to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;add&#34;;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_dropout"><code class="name flex">
<span>def <span class="ident">append_dropout</span></span>(<span>self, probability=0.5, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append dropout layer to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>probability</code></strong> :&ensp;<code>float</code></dt>
<dd>Droping probability of neurons in next layer</dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_dropout(self, probability=0.5, final_layer=False):
    &#39;&#39;&#39;
    Append dropout layer to base network in transfer learning

    Args:
        probability (float): Droping probability of neurons in next layer
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = layer_dropout(self.system_dict, probability=probability, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_elu"><code class="name flex">
<span>def <span class="ident">append_elu</span></span>(<span>self, alpha=1.0, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append exponential linear unit activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplicatve factor.</dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_elu(self, alpha=1.0, final_layer=False):
    &#39;&#39;&#39;
    Append exponential linear unit activation to base network in transfer learning

    Args:
        alpha (float): Multiplicatve factor.
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_elu(self.system_dict, alpha=alpha, final_layer=final_layer); </code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_leakyrelu"><code class="name flex">
<span>def <span class="ident">append_leakyrelu</span></span>(<span>self, negative_slope=0.01, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Leaky - ReLU activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>negative_slope</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplicatve factor towards negative spectrum of real numbers.</dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_leakyrelu(self, negative_slope=0.01, final_layer=False):
    &#39;&#39;&#39;
    Append Leaky - ReLU activation to base network in transfer learning

    Args:
        negative_slope (float): Multiplicatve factor towards negative spectrum of real numbers.
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_leakyrelu(self.system_dict, negative_slope=negative_slope, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_linear"><code class="name flex">
<span>def <span class="ident">append_linear</span></span>(<span>self, num_neurons=False, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append dense (fully connected) layer to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_neurons</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neurons in the dense layer</dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, then number of neurons are directly set as number of classes in dataset for single label type classification</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_linear(self, num_neurons=False, final_layer=False):
    &#39;&#39;&#39;
    Append dense (fully connected) layer to base network in transfer learning

    Args:
        num_neurons (int): Number of neurons in the dense layer
        final_layer (bool): If True, then number of neurons are directly set as number of classes in dataset for single label type classification

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        if(not num_neurons):
            num_neurons = self.system_dict[&#34;dataset&#34;][&#34;params&#34;][&#34;num_classes&#34;];
        self.system_dict = layer_linear(self.system_dict, num_neurons=num_neurons, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_prelu"><code class="name flex">
<span>def <span class="ident">append_prelu</span></span>(<span>self, num_parameters=1, init=0.25, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Learnable parameerized rectified linear unit activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>init</code></strong> :&ensp;<code>float</code></dt>
<dd>Initialization value for multiplicatve factor towards negative spectrum of real numbers.</dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_prelu(self, num_parameters=1, init=0.25, final_layer=False):
    &#39;&#39;&#39;
    Append Learnable parameerized rectified linear unit activation to base network in transfer learning

    Args:
        init (float): Initialization value for multiplicatve factor towards negative spectrum of real numbers.
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_prelu(self.system_dict, num_parameters=num_parameters, init=init, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_relu"><code class="name flex">
<span>def <span class="ident">append_relu</span></span>(<span>self, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append rectified linear unit activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_relu(self, final_layer=False):
    &#39;&#39;&#39;
    Append rectified linear unit activation to base network in transfer learning

    Args:
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_relu(self.system_dict, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_selu"><code class="name flex">
<span>def <span class="ident">append_selu</span></span>(<span>self, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append scaled exponential linear unit activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_selu(self, final_layer=False):
    &#39;&#39;&#39;
    Append scaled exponential linear unit activation to base network in transfer learning

    Args:
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_selu(self.system_dict, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_sigmoid"><code class="name flex">
<span>def <span class="ident">append_sigmoid</span></span>(<span>self, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append sigmoid activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_sigmoid(self, final_layer=False):
    &#39;&#39;&#39;
    Append sigmoid activation to base network in transfer learning

    Args:
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_sigmoid(self.system_dict, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_softplus"><code class="name flex">
<span>def <span class="ident">append_softplus</span></span>(<span>self, beta=1, threshold=20, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append softplus activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>beta</code></strong> :&ensp;<code>int</code></dt>
<dd>Multiplicative factor</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>int</code></dt>
<dd>softplus (thresholded relu) limit </dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_softplus(self, beta=1, threshold=20, final_layer=False):
    &#39;&#39;&#39;
    Append softplus activation to base network in transfer learning

    Args:
        beta (int): Multiplicative factor
        threshold (int): softplus (thresholded relu) limit 
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_softplus(self.system_dict, beta=1, threshold=20, final_layer=False);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_softsign"><code class="name flex">
<span>def <span class="ident">append_softsign</span></span>(<span>self, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append softsign activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_softsign(self, final_layer=False):
    &#39;&#39;&#39;
    Append softsign activation to base network in transfer learning

    Args:
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_softsign(self.system_dict, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_swish"><code class="name flex">
<span>def <span class="ident">append_swish</span></span>(<span>self, beta=1.0, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append swish activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>beta</code></strong> :&ensp;<code>bool</code></dt>
<dd>Multiplicative factor</dd>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_swish(self, beta=1.0, final_layer=False):
    &#39;&#39;&#39;
    Append swish activation to base network in transfer learning

    Args:
        beta (bool): Multiplicative factor
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_swish(self.system_dict, beta=beta, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_tanh"><code class="name flex">
<span>def <span class="ident">append_tanh</span></span>(<span>self, final_layer=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Append tanh activation to base network in transfer learning</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>final_layer</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicator that this layer marks the end of network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_tanh(self, final_layer=False):
    &#39;&#39;&#39;
    Append tanh activation to base network in transfer learning

    Args:
        final_layer (bool): Indicator that this layer marks the end of network.

    Returns:
        None
    &#39;&#39;&#39;
    if(self.system_dict[&#34;model&#34;][&#34;final_layer&#34;]):
        msg = &#34;Cannot append more layers.\n&#34;;
        msg += &#34;Tip: Previously appended layer termed as final layer&#34;;
        raise ConstraintError(msg);
    else:
        self.system_dict = activation_tanh(self.system_dict, final_layer=final_layer);</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling"><code class="name flex">
<span>def <span class="ident">average_pooling</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, include_padding_in_calculation=True, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>include_padding_in_calculation</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, padding will be considered.</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def average_pooling(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
    layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-average-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        include_padding_in_calculation (bool): If True, padding will be considered.
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;average_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling1d"><code class="name flex">
<span>def <span class="ident">average_pooling1d</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, include_padding_in_calculation=True, layout='NCW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 1d-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>include_padding_in_calculation</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, padding will be considered.</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCW' - order
2) 'NWC' - order
- N: Number of elements in batches
- C: Number of channels
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def average_pooling1d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
    layout=&#39;NCW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 1d-average-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        include_padding_in_calculation (bool): If True, padding will be considered.
        ceil_mode (bool): If True, apply ceil math operation post pooling
        layout (str): Either of these values (order)
                        1) &#39;NCW&#39; - order
                        2) &#39;NWC&#39; - order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;average_pooling1d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling2d"><code class="name flex">
<span>def <span class="ident">average_pooling2d</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, include_padding_in_calculation=True, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>include_padding_in_calculation</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, padding will be considered.</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def average_pooling2d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
    layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-average-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        include_padding_in_calculation (bool): If True, padding will be considered.
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;average_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling3d"><code class="name flex">
<span>def <span class="ident">average_pooling3d</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, include_padding_in_calculation=True, layout='NCDHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 3d-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>include_padding_in_calculation</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, padding will be considered.</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCDHW' - Order
2) 'NDHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- D: Depth of features in layers
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def average_pooling3d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, include_padding_in_calculation=True, 
    layout=&#39;NCDHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 3d-average-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        include_padding_in_calculation (bool): If True, padding will be considered.
        layout (str): Either of these values (order)
                        1) &#39;NCDHW&#39; - Order
                        2) &#39;NDHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - D: Depth of features in layers
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;average_pooling3d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;include_padding_in_calculation&#34;] = include_padding_in_calculation;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;  
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.batch_normalization"><code class="name flex">
<span>def <span class="ident">batch_normalization</span></span>(<span>self, moving_average_momentum=0.9, epsilon=1e-05, use_trainable_parameters=True, activate_scale_shift_operation=False, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append batch normalization layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>moving_average_momentum</code></strong> :&ensp;<code>float</code></dt>
<dd>Normalization momentum value</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Value to avoid division by zero</dd>
<dt><strong><code>use_trainable_paramemetrs</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, batch norm turns into a trainable layer</dd>
<dt><strong><code>activate_scale_shift_operation</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed status - False</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
    activate_scale_shift_operation=False, uid=None):
    &#39;&#39;&#39;
    Append batch normalization layer to custom network

    Args:
        moving_average_momentum (float): Normalization momentum value
        epsilon (float): Value to avoid division by zero
        use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
        activate_scale_shift_operation (bool): Fixed status - False
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;batch_normalization&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;moving_average_momentum&#34;] = moving_average_momentum;
    tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
    tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
    tmp[&#34;params&#34;][&#34;activate_scale_shift_operation&#34;] = activate_scale_shift_operation;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.concatenate"><code class="name flex">
<span>def <span class="ident">concatenate</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append concatenation layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concatenate(self, uid=None):
    &#39;&#39;&#39;
    Append concatenation layer to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;concatenate&#34;;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.conv_bn_relu_block"><code class="name flex">
<span>def <span class="ident">conv_bn_relu_block</span></span>(<span>self, output_channels=64, kernel_size=1, stride=1, padding=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Conv-&gt;batch_norm-&gt;relu Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Kernel matrix shape for all layers in this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>external zero padding on input</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_bn_relu_block(self, output_channels=64, kernel_size=1, stride=1, padding=None):
    &#39;&#39;&#39;
    Append Conv-&gt;batch_norm-&gt;relu Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        kernel_size (int): Kernel matrix shape for all layers in this block
        stride (int): kernel movement stride  
        padding (int, tuple): external zero padding on input

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    if(padding):
        network.append(self.convolution(output_channels=output_channels, 
                                       kernel_size=kernel_size, 
                                       stride=stride,
                                       padding=padding));
    else:
        network.append(self.convolution(output_channels=output_channels, 
                                       kernel_size=kernel_size, 
                                       stride=stride));
    network.append(self.batch_normalization());
    network.append(self.relu());
    
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution"><code class="name flex">
<span>def <span class="ident">convolution</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', groups=1, dilation=1, use_bias=True, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolution(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;convolution2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution1d"><code class="name flex">
<span>def <span class="ident">convolution1d</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', groups=1, dilation=1, use_bias=True, layout='NCW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 1d-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCW' - order
2) 'NWC' - order
- N: Number of elements in batches
- C: Number of channels
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolution1d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    groups=1, dilation=1, use_bias=True, layout=&#39;NCW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 1d-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCW&#39; - order
                        2) &#39;NWC&#39; - order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;convolution1d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution2d"><code class="name flex">
<span>def <span class="ident">convolution2d</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', groups=1, dilation=1, use_bias=True, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolution2d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;convolution2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution3d"><code class="name flex">
<span>def <span class="ident">convolution3d</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', groups=1, dilation=1, use_bias=True, layout='NCDHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 3d-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCDHW' - Order
2) 'NDHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- D: Depth of features in layers
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolution3d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    groups=1, dilation=1, use_bias=True, layout=&#39;NCDHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 3d-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCDHW&#39; - Order
                        2) &#39;NDHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - D: Depth of features in layers
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;convolution3d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.densenet_block"><code class="name flex">
<span>def <span class="ident">densenet_block</span></span>(<span>self, bottleneck_size=4, growth_rate=16, dropout=0.2)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Densenet Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>bottleneck_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Bottleneck dimensions for reducing number of features</dd>
<dt><strong><code>growth_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Expansion rate for convolution layers for this block</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code></dt>
<dd>Prbability for dropout layer post convolution</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def densenet_block(self, bottleneck_size=4, growth_rate=16, dropout=0.2):
    &#39;&#39;&#39;
    Append Densenet Block to custom network

    Args:
        bottleneck_size (int): Bottleneck dimensions for reducing number of features
        growth_rate (int): Expansion rate for convolution layers for this block
        dropout (float): Prbability for dropout layer post convolution

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=bottleneck_size*growth_rate, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=growth_rate, kernel_size=3, stride=1));
    branch_1.append(self.dropout(drop_probability=dropout));
    
    branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);
    
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.dropout"><code class="name flex">
<span>def <span class="ident">dropout</span></span>(<span>self, drop_probability=0.2, axes=(), uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append dropout layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drop_probability</code></strong> :&ensp;<code>float</code></dt>
<dd>Probability for not considering neurons in the output</dd>
<dt><strong><code>axes</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Channel axis to implement dropout over</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dropout(self, drop_probability=0.2, axes=(), uid=None):
    &#39;&#39;&#39;
    Append dropout layer to custom network

    Args:
        drop_probability (float): Probability for not considering neurons in the output
        axes (tuple): Channel axis to implement dropout over
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;dropout&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;drop_probability&#34;] = drop_probability;
    tmp[&#34;params&#34;][&#34;axes&#34;] = axes;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.elu"><code class="name flex">
<span>def <span class="ident">elu</span></span>(<span>self, alpha=1.0, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append exponential linear unit activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplicative factor</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def elu(self, alpha=1.0, uid=None): 
    &#39;&#39;&#39;
    Append exponential linear unit activation to custom network

    Args:
        alpha (float): Multiplicative factor
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;elu&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;alpha&#34;] = alpha;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append flatten layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten(self, uid=None):
    &#39;&#39;&#39;
    Append flatten layer to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;flatten&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.fully_connected"><code class="name flex">
<span>def <span class="ident">fully_connected</span></span>(<span>self, units=512, use_bias=True, flatten=True, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append fully-connected (dense) layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>units</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of neurons in the layer</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>flatten</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed to True</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fully_connected(self, units=512, use_bias=True, flatten=True, uid=None):
    &#39;&#39;&#39;
    Append fully-connected (dense) layer to custom network

    Args:
        units (int): Number of neurons in the layer
        use_bias (bool): If True, learnable bias is added
        flatten (bool): Fixed to True
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;fully_connected&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;units&#34;] = units; 
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias; 
    tmp[&#34;params&#34;][&#34;flatten&#34;] = flatten; 
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.gelu"><code class="name flex">
<span>def <span class="ident">gelu</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append gated exponential linear unit activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gelu(self, uid=None): 
    &#39;&#39;&#39;
    Append gated exponential linear unit activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;gelu&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling"><code class="name flex">
<span>def <span class="ident">global_average_pooling</span></span>(<span>self, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-global-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_average_pooling(self, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-global-average-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_average_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling1d"><code class="name flex">
<span>def <span class="ident">global_average_pooling1d</span></span>(<span>self, layout='NCW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 1d-global-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCW' - order
2) 'NWC' - order
- N: Number of elements in batches
- C: Number of channels
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_average_pooling1d(self, layout=&#39;NCW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 1d-global-average-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCW&#39; - order
                        2) &#39;NWC&#39; - order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_average_pooling1d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling2d"><code class="name flex">
<span>def <span class="ident">global_average_pooling2d</span></span>(<span>self, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-global-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_average_pooling2d(self, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-global-average-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_average_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling3d"><code class="name flex">
<span>def <span class="ident">global_average_pooling3d</span></span>(<span>self, layout='NCDHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 3d-global-average-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCDHW' - Order
2) 'NDHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- D: Depth of features in layers
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_average_pooling3d(self, layout=&#39;NCDHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 3d-global-average-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCDHW&#39; - Order
                        2) &#39;NDHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - D: Depth of features in layers
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_average_pooling3d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling"><code class="name flex">
<span>def <span class="ident">global_max_pooling</span></span>(<span>self, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-global-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_max_pooling(self, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-global-max-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_max_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling1d"><code class="name flex">
<span>def <span class="ident">global_max_pooling1d</span></span>(<span>self, layout='NCW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 1d-global-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCW' - order
2) 'NWC' - order
- N: Number of elements in batches
- C: Number of channels
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_max_pooling1d(self, layout=&#39;NCW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 1d-global-max-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCW&#39; - order
                        2) &#39;NWC&#39; - order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_max_pooling1d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling2d"><code class="name flex">
<span>def <span class="ident">global_max_pooling2d</span></span>(<span>self, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-global-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_max_pooling2d(self, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-global-max-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_max_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp; </code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling3d"><code class="name flex">
<span>def <span class="ident">global_max_pooling3d</span></span>(<span>self, layout='NCDHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 3d-global-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCDHW' - Order
2) 'NDHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- D: Depth of features in layers
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_max_pooling3d(self, layout=&#39;NCDHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 3d-global-max-pooling to custom network

    Args:
        layout (str): Either of these values (order)
                        1) &#39;NCDHW&#39; - Order
                        2) &#39;NDHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - D: Depth of features in layers
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;global_max_pooling3d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout; 
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.identity"><code class="name flex">
<span>def <span class="ident">identity</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append identity layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def identity(self, uid=None):
    &#39;&#39;&#39;
    Append identity layer to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;identity&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_a_block"><code class="name flex">
<span>def <span class="ident">inception_a_block</span></span>(<span>self, pooling_branch_channels=32, pool_type='avg')</span>
</code></dt>
<dd>
<section class="desc"><p>Append Inception-A Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pooling_branch_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features for conv layers in pooling branch</dd>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these types
- "avg" - Average pooling
- "max" - Max pooling</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inception_a_block(self, pooling_branch_channels=32, pool_type=&#34;avg&#34;):
    &#39;&#39;&#39;
    Append Inception-A Block to custom network

    Args:
        pooling_branch_channels (int): Number of features for conv layers in pooling branch
        pool_type (str): Either of these types
                            - &#34;avg&#34; - Average pooling
                            - &#34;max&#34; - Max pooling
        
    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    branch_3 = [];
    branch_4 = [];
    
    branch_1.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1))
    
    branch_2.append(self.conv_bn_relu_block(output_channels=48, kernel_size=1));
    branch_2.append(self.conv_bn_relu_block(output_channels=64, kernel_size=5));
       
    branch_3.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1));
    branch_3.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
    branch_3.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
    
    if(pool_type==&#34;avg&#34;):
        branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
    else:
        branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
    branch_4.append(self.conv_bn_relu_block(output_channels=pooling_branch_channels, kernel_size=1));
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(branch_3);
    subnetwork.append(branch_4);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);
    
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_b_block"><code class="name flex">
<span>def <span class="ident">inception_b_block</span></span>(<span>self, pool_type='avg')</span>
</code></dt>
<dd>
<section class="desc"><p>Append Inception-B Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these types
- "avg" - Average pooling
- "max" - Max pooling</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inception_b_block(self, pool_type=&#34;avg&#34;):
    &#39;&#39;&#39;
    Append Inception-B Block to custom network

    Args:
        pool_type (str): Either of these types
                            - &#34;avg&#34; - Average pooling
                            - &#34;max&#34; - Max pooling
        
    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    branch_3 = [];
    branch_4 = [];
    
    branch_1.append(self.conv_bn_relu_block(output_channels=384, kernel_size=3))
          
    branch_2.append(self.conv_bn_relu_block(output_channels=64, kernel_size=1));
    branch_2.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
    branch_2.append(self.conv_bn_relu_block(output_channels=96, kernel_size=3));
    
    if(pool_type==&#34;avg&#34;):
        branch_3.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
    else:
        branch_3.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(branch_3);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);

    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_c_block"><code class="name flex">
<span>def <span class="ident">inception_c_block</span></span>(<span>self, channels_7x7=3, pool_type='avg')</span>
</code></dt>
<dd>
<section class="desc"><p>Append Inception-C Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>channels_7x7</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features for conv layers in channels_7x7 branch</dd>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these types
- "avg" - Average pooling
- "max" - Max pooling</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inception_c_block(self, channels_7x7=3, pool_type=&#34;avg&#34;):
    &#39;&#39;&#39;
    Append Inception-C Block to custom network

    Args:
        channels_7x7 (int): Number of features for conv layers in channels_7x7 branch
        pool_type (str): Either of these types
                            - &#34;avg&#34; - Average pooling
                            - &#34;max&#34; - Max pooling
        
    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    branch_3 = [];
    branch_4 = [];
    
    branch_1.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1))
    
    
    branch_2.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=1));
    branch_2.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
    branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
       
        
    branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=1));
    branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
    branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(7, 1), padding=(3, 0)));
    branch_3.append(self.conv_bn_relu_block(output_channels=channels_7x7, kernel_size=(1, 7), padding=(0, 3)));
    branch_3.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
    
    if(pool_type==&#34;avg&#34;):
        branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
    else:
        branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
    branch_4.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(branch_3);
    subnetwork.append(branch_4);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);
    
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_d_block"><code class="name flex">
<span>def <span class="ident">inception_d_block</span></span>(<span>self, pool_type='avg')</span>
</code></dt>
<dd>
<section class="desc"><p>Append Inception-D Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these types
- "avg" - Average pooling
- "max" - Max pooling</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inception_d_block(self, pool_type=&#34;avg&#34;):
    &#39;&#39;&#39;
    Append Inception-D Block to custom network

    Args:
        pool_type (str): Either of these types
                            - &#34;avg&#34; - Average pooling
                            - &#34;max&#34; - Max pooling
        
    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    branch_3 = [];
    branch_4 = [];
    
    branch_1.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1))
    branch_1.append(self.conv_bn_relu_block(output_channels=320, kernel_size=3, stride=2))
    
    
    branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
    branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(1, 7), padding=(0, 3)));
    branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=(7, 1), padding=(3, 0)));
    branch_2.append(self.conv_bn_relu_block(output_channels=192, kernel_size=3, stride=2));

    
    if(pool_type==&#34;avg&#34;):
        branch_3.append(self.average_pooling(kernel_size=3, stride=2));
    else:
        branch_3.append(self.max_pooling(kernel_size=3, stride=2));
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(branch_3);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);
    
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_e_block"><code class="name flex">
<span>def <span class="ident">inception_e_block</span></span>(<span>self, pool_type='avg')</span>
</code></dt>
<dd>
<section class="desc"><p>Append Inception-C Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these types
- "avg" - Average pooling
- "max" - Max pooling</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inception_e_block(self, pool_type=&#34;avg&#34;):
    &#39;&#39;&#39;
    Append Inception-C Block to custom network

    Args:
        pool_type (str): Either of these types
                            - &#34;avg&#34; - Average pooling
                            - &#34;max&#34; - Max pooling
        
    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    branch_3 = [];
    branch_4 = [];
    
    branch_1.append(self.conv_bn_relu_block(output_channels=320, kernel_size=1))
    
    branch_2.append(self.conv_bn_relu_block(output_channels=384, kernel_size=1));
    branch_2.append(self.subbranch_block());
    
    
    branch_3.append(self.conv_bn_relu_block(output_channels=448, kernel_size=1));
    branch_3.append(self.conv_bn_relu_block(output_channels=384, kernel_size=3, padding=1));
    branch_3.append(self.subbranch_block());
    

    
    if(pool_type==&#34;avg&#34;):
        branch_4.append(self.average_pooling(kernel_size=3, stride=1, padding=1));
    else:
        branch_4.append(self.max_pooling(kernel_size=3, stride=1, padding=1));
    branch_4.append(self.conv_bn_relu_block(output_channels=192, kernel_size=1));
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(branch_3);
    subnetwork.append(branch_4);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);
    
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.layer_normalization"><code class="name flex">
<span>def <span class="ident">layer_normalization</span></span>(<span>self, moving_average_momentum=0.9, epsilon=1e-05, use_trainable_parameters=True, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append layer normalization layer to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>moving_average_momentum</code></strong> :&ensp;<code>float</code></dt>
<dd>Normalization momentum value</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Value to avoid division by zero</dd>
<dt><strong><code>use_trainable_paramemetrs</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, batch norm turns into a trainable layer</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_normalization(self, moving_average_momentum=0.9, epsilon=0.00001, use_trainable_parameters=True, 
    uid=None):
    &#39;&#39;&#39;
    Append layer normalization layer to custom network

    Args:
        moving_average_momentum (float): Normalization momentum value
        epsilon (float): Value to avoid division by zero
        use_trainable_paramemetrs (bool): If True, batch norm turns into a trainable layer
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;layer_normalization&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;epsilon&#34;] = epsilon;
    tmp[&#34;params&#34;][&#34;use_trainable_parameters&#34;] = use_trainable_parameters;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.leaky_relu"><code class="name flex">
<span>def <span class="ident">leaky_relu</span></span>(<span>self, alpha=0.01, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append leaky relu activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplicatve factor towards negative spectrum of real numbers.</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def leaky_relu(self, alpha=0.01, uid=None): 
    &#39;&#39;&#39;
    Append leaky relu activation to custom network

    Args:
        alpha (float): Multiplicatve factor towards negative spectrum of real numbers.
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;leaky_relu&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;alpha&#34;] = alpha;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling"><code class="name flex">
<span>def <span class="ident">max_pooling</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_pooling(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-max-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;max_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling1d"><code class="name flex">
<span>def <span class="ident">max_pooling1d</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, layout='NCW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 1d-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCW' - order
2) 'NWC' - order
- N: Number of elements in batches
- C: Number of channels
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_pooling1d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, layout=&#39;NCW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 1d-max-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        layout (str): Either of these values (order)
                        1) &#39;NCW&#39; - order
                        2) &#39;NWC&#39; - order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;max_pooling1d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling2d"><code class="name flex">
<span>def <span class="ident">max_pooling2d</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_pooling2d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-max-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;max_pooling2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling3d"><code class="name flex">
<span>def <span class="ident">max_pooling3d</span></span>(<span>self, kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, layout='NCDHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 3d-max-pooling to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated pooling</dd>
<dt><strong><code>return_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>Fixed value set as False</dd>
<dt><strong><code>ceil_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, apply ceil math operation post pooling</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCDHW' - Order
2) 'NDHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- D: Depth of features in layers
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_pooling3d(self, kernel_size=2, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False, layout=&#39;NCDHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 3d-max-pooling to custom network

    Args:
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        dilation (int): Factor for dilated pooling
        return_indices (bool): Fixed value set as False
        ceil_mode (bool): If True, apply ceil math operation post pooling
        layout (str): Either of these values (order)
                        1) &#39;NCDHW&#39; - Order
                        2) &#39;NDHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - D: Depth of features in layers
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;max_pooling3d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;return_indices&#34;] = return_indices;
    tmp[&#34;params&#34;][&#34;ceil_mode&#34;] = ceil_mode;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.mobilenet_v2_inverted_linear_bottleneck_block"><code class="name flex">
<span>def <span class="ident">mobilenet_v2_inverted_linear_bottleneck_block</span></span>(<span>self, output_channels=32, bottleneck_width=4, stride=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Mobilenet V2 Inverted Linear Bottleneck Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>bottleneck_width</code></strong> :&ensp;<code>int</code></dt>
<dd>Bottleneck dimensions for reducing number of features</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mobilenet_v2_inverted_linear_bottleneck_block(self, output_channels=32, bottleneck_width=4, stride=1):
    &#39;&#39;&#39;
    Append Mobilenet V2 Inverted Linear Bottleneck Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        stride (int): kernel movement stride  
        bottleneck_width (int): Bottleneck dimensions for reducing number of features

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    if(bottleneck_width != 1):
        branch_1.append(self.convolution(output_channels=output_channels//bottleneck_width, 
                                        kernel_size=1, stride=1));
    
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    sep_conv = self.separable_convolution_block(input_channels=output_channels//bottleneck_width,
                                                    output_channels=output_channels//bottleneck_width,
                                                    kernel_size=3, stride=stride);
    branch_1.append(sep_conv);   
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    
    
    branch_2 = [];
    branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork);
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.mobilenet_v2_linear_bottleneck_block"><code class="name flex">
<span>def <span class="ident">mobilenet_v2_linear_bottleneck_block</span></span>(<span>self, output_channels=32, bottleneck_width=4, stride=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Mobilenet V2 Linear Bottleneck Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>bottleneck_width</code></strong> :&ensp;<code>int</code></dt>
<dd>Bottleneck dimensions for reducing number of features</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mobilenet_v2_linear_bottleneck_block(self, output_channels=32, bottleneck_width=4, stride=1):
    &#39;&#39;&#39;
    Append Mobilenet V2 Linear Bottleneck Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        stride (int): kernel movement stride  
        bottleneck_width (int): Bottleneck dimensions for reducing number of features

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    subnetwork = [];
    branch_1 = [];
    if(bottleneck_width != 1):
        branch_1.append(self.convolution(output_channels=output_channels*bottleneck_width, 
                                        kernel_size=1, stride=1));
    
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels*bottleneck_width,
                                    kernel_size=3, stride=stride));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    
    
    branch_2 = [];
    branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork);
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.prelu"><code class="name flex">
<span>def <span class="ident">prelu</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append paramemeterized rectified linear unit activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prelu(self, uid=None): 
    &#39;&#39;&#39;
    Append paramemeterized rectified linear unit activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;prelu&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.relu"><code class="name flex">
<span>def <span class="ident">relu</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append rectified linear unit activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def relu(self, uid=None):
    &#39;&#39;&#39;
    Append rectified linear unit activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;relu&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v1_block"><code class="name flex">
<span>def <span class="ident">resnet_v1_block</span></span>(<span>self, output_channels=16, stride=1, downsample=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Resnet V1 Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>bool</code></dt>
<dd>If False, residual branch is a shortcut,
Else, residual branch has non-identity layers</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_v1_block(self, output_channels=16, stride=1, downsample=True):
    &#39;&#39;&#39;
    Append Resnet V1 Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        stride (int, tuple): kernel movement stride  
        downsample (bool): If False, residual branch is a shortcut,
                            Else, residual branch has non-identity layers

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];

    subnetwork = [];
    branch_1 = [];
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=stride));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=1));
    branch_1.append(self.batch_normalization());
    
    branch_2 = [];
    if(downsample):
        branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        branch_2.append(self.batch_normalization());
    else:
        branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork)
    network.append(self.relu());
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v1_bottleneck_block"><code class="name flex">
<span>def <span class="ident">resnet_v1_bottleneck_block</span></span>(<span>self, output_channels=16, stride=1, downsample=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Resnet V1 Bottleneck Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>bool</code></dt>
<dd>If False, residual branch is a shortcut,
Else, residual branch has non-identity layers</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_v1_bottleneck_block(self, output_channels=16, stride=1, downsample=True):
    &#39;&#39;&#39;
    Append Resnet V1 Bottleneck Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        stride (int, tuple): kernel movement stride  
        downsample (bool): If False, residual branch is a shortcut,
                            Else, residual branch has non-identity layers

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];

    subnetwork = [];
    branch_1 = [];
    branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=1, stride=stride));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=3, stride=1));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    
    branch_2 = [];
    if(downsample):
        branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        branch_2.append(self.batch_normalization());
    else:
        branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork)
    network.append(self.relu())
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v2_block"><code class="name flex">
<span>def <span class="ident">resnet_v2_block</span></span>(<span>self, output_channels=16, stride=1, downsample=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Resnet V2 Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>bool</code></dt>
<dd>If False, residual branch is a shortcut,
Else, residual branch has non-identity layers</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_v2_block(self, output_channels=16, stride=1, downsample=True):
    &#39;&#39;&#39;
    Append Resnet V2 Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        stride (int, tuple): kernel movement stride  
        downsample (bool): If False, residual branch is a shortcut,
                            Else, residual branch has non-identity layers

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    network.append(self.batch_normalization());
    network.append(self.relu());
    
    subnetwork = [];
    branch_1 = [];
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=stride));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=3, stride=1));
    
    branch_2 = [];
    if(downsample):
        branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
    else:
        branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork);
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v2_bottleneck_block"><code class="name flex">
<span>def <span class="ident">resnet_v2_bottleneck_block</span></span>(<span>self, output_channels=16, stride=1, downsample=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Resnet V2 Bottleneck Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>bool</code></dt>
<dd>If False, residual branch is a shortcut,
Else, residual branch has non-identity layers</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_v2_bottleneck_block(self, output_channels=16, stride=1, downsample=True):
    &#39;&#39;&#39;
    Append Resnet V2 Bottleneck Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        stride (int): kernel movement stride  
        downsample (bool): If False, residual branch is a shortcut,
                            Else, residual branch has non-identity layers

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    network.append(self.batch_normalization());
    network.append(self.relu());
    
    subnetwork = [];
    branch_1 = [];
    branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels//4, kernel_size=3, stride=stride));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
    
    branch_2 = [];
    if(downsample):
        branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
    else:
        branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork)
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnext_block"><code class="name flex">
<span>def <span class="ident">resnext_block</span></span>(<span>self, output_channels=256, cardinality=8, bottleneck_width=4, stride=1, downsample=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Resnext Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>cardinality</code></strong> :&ensp;<code>int</code></dt>
<dd>cardinality dimensions for complex transformations</dd>
<dt><strong><code>bottleneck_width</code></strong> :&ensp;<code>int</code></dt>
<dd>Bottleneck dimensions for reducing number of features</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>bool</code></dt>
<dd>If False, residual branch is a shortcut,
Else, residual branch has non-identity layers</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnext_block(self, output_channels=256, cardinality=8, bottleneck_width=4, stride=1, downsample=True):
    &#39;&#39;&#39;
    Append Resnext Block to custom network

    Args:
        output_channels (int): Number of output features for this block
        cardinality (int): cardinality dimensions for complex transformations
        bottleneck_width (int): Bottleneck dimensions for reducing number of features
        stride (int): kernel movement stride  
        downsample (bool): If False, residual branch is a shortcut,
                            Else, residual branch has non-identity layers

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];

    channels = output_channels//4;
    D = int(math.floor(channels * (bottleneck_width / 64)))
    group_width = cardinality * D
    
    subnetwork = [];
    branch_1 = [];
    branch_1.append(self.convolution(output_channels=group_width, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=group_width, kernel_size=3, stride=stride));
    branch_1.append(self.batch_normalization());
    branch_1.append(self.relu());
    branch_1.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=1));
    branch_1.append(self.batch_normalization());
    
    
    
    branch_2 = [];
    if(downsample):
        branch_2.append(self.convolution(output_channels=output_channels, kernel_size=1, stride=stride));
        branch_2.append(self.batch_normalization());
    else:
        branch_2.append(self.identity());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.add());
    
    network.append(subnetwork)
    network.append(self.relu());
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.selu"><code class="name flex">
<span>def <span class="ident">selu</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append scaled exponential linear unit activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def selu(self, uid=None): 
    &#39;&#39;&#39;
    Append scaled exponential linear unit activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;selu&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.separable_convolution_block"><code class="name flex">
<span>def <span class="ident">separable_convolution_block</span></span>(<span>self, input_channels=16, output_channels=32, kernel_size=3, stride=1, padding=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Separable convolution Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input features for this block</dd>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Kernel matrix shape for all layers in this block</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>external zero padding on input</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def separable_convolution_block(self, input_channels=16, output_channels=32, kernel_size=3, stride=1, padding=1):
    &#39;&#39;&#39;
    Append Separable convolution Block to custom network

    Args:
        input_channels (int): Number of input features for this block
        output_channels (int): Number of output features for this block
        kernel_size (int): Kernel matrix shape for all layers in this block
        stride (int): kernel movement stride  
        padding (int, tuple): external zero padding on input

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    network.append(self.convolution(output_channels=input_channels, kernel_size=kernel_size, 
                                   stride=stride, padding=padding, groups=input_channels));
    network.append(self.convolution(output_channels=output_channels, kernel_size=1, 
                                   stride=1));

    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append sigmoid activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoid(self, uid=None):
    &#39;&#39;&#39;
    Append sigmoid activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;sigmoid&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.softplus"><code class="name flex">
<span>def <span class="ident">softplus</span></span>(<span>self, beta=1, threshold=20, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append softplus activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>beta</code></strong> :&ensp;<code>int</code></dt>
<dd>Multiplicative factor</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>int</code></dt>
<dd>softplus (thresholded relu) limit </dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def softplus(self, beta=1, threshold=20, uid=None): 
    &#39;&#39;&#39;
    Append softplus activation to custom network

    Args:
        beta (int): Multiplicative factor
        threshold (int): softplus (thresholded relu) limit 
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;softplus&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;beta&#34;] = beta;
    tmp[&#34;params&#34;][&#34;threshold&#34;] = threshold;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.softsign"><code class="name flex">
<span>def <span class="ident">softsign</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append softsign activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def softsign(self, uid=None): 
    &#39;&#39;&#39;
    Append softsign activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;softsign&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.squeezenet_fire_block"><code class="name flex">
<span>def <span class="ident">squeezenet_fire_block</span></span>(<span>self, squeeze_channels=16, expand_channels_1x1=32, expand_channels_3x3=64)</span>
</code></dt>
<dd>
<section class="desc"><p>Append Squeezenet Fire Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>squeeze_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this block</dd>
<dt><strong><code>expand_channels_1x1</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of convolution_1x1 features for this block</dd>
<dt><strong><code>expand_channels_3x3</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of convolution_3x3 features for this block</dd>
<dt><strong><code>bottleneck_width</code></strong> :&ensp;<code>int</code></dt>
<dd>Bottleneck dimensions for reducing number of features</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def squeezenet_fire_block(self, squeeze_channels=16, expand_channels_1x1=32, expand_channels_3x3=64):
    &#39;&#39;&#39;
    Append Squeezenet Fire Block to custom network

    Args:
        squeeze_channels (int): Number of output features for this block
        expand_channels_1x1 (int): Number of convolution_1x1 features for this block
        expand_channels_3x3 (int): Number of convolution_3x3 features for this block
        bottleneck_width (int): Bottleneck dimensions for reducing number of features

    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;
    network = [];
    
    network.append(self.convolution(output_channels=squeeze_channels, kernel_size=1, stride=1));
    network.append(self.relu());
    
    subnetwork = [];
    branch_1 = [];    
    branch_2 = [];
    
    branch_1.append(self.convolution(output_channels=expand_channels_1x1, kernel_size=1, stride=1));
    branch_1.append(self.relu());
    
    branch_2.append(self.convolution(output_channels=expand_channels_3x3, kernel_size=3, stride=1));
    branch_2.append(self.relu());
    
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.concatenate());
    
    network.append(subnetwork);
    return network;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.subbranch_block"><code class="name flex">
<span>def <span class="ident">subbranch_block</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Append sub-branch Block to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>Containing all the layer dictionaries arranged as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subbranch_block(self):  
    &#39;&#39;&#39;
    Append sub-branch Block to custom network

    Args:
        None
        
    Returns:
        list: Containing all the layer dictionaries arranged as per function arguments
    &#39;&#39;&#39;  
    subnetwork = [];
    branch_1 = [];
    branch_2 = [];
    branch_1.append(self.conv_bn_relu_block(output_channels=384, kernel_size=(1, 3), padding=(0, 1)));
    branch_2.append(self.conv_bn_relu_block(output_channels=384, kernel_size=(3, 1), padding=(1, 0)));
    subnetwork.append(branch_1);
    subnetwork.append(branch_2);
    subnetwork.append(self.concatenate());
    return subnetwork;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.swish"><code class="name flex">
<span>def <span class="ident">swish</span></span>(<span>self, beta=1.0, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append swish activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>beta</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplicative factor</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def swish(self, beta=1.0, uid=None): 
    &#39;&#39;&#39;
    Append swish activation to custom network

    Args:
        beta (float): Multiplicative factor
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;swish&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;beta&#34;] = beta;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.tanh"><code class="name flex">
<span>def <span class="ident">tanh</span></span>(<span>self, uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append tanh activation to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tanh(self, uid=None):
    &#39;&#39;&#39;
    Append tanh activation to custom network

    Args:
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp = {};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;tanh&#34;;
    tmp[&#34;params&#34;] = {};
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution"><code class="name flex">
<span>def <span class="ident">transposed_convolution</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', output_padding=0, groups=1, dilation=1, use_bias=True, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-transposed-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>output_padding</code></strong> :&ensp;<code>int</code></dt>
<dd>Additional padding applied to output</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transposed_convolution(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-transposed-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        output_padding (int): Additional padding applied to output
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;transposed_convolution2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution1d"><code class="name flex">
<span>def <span class="ident">transposed_convolution1d</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', output_padding=0, groups=1, dilation=1, use_bias=True, layout='NCW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 1d-transposed-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>output_padding</code></strong> :&ensp;<code>int</code></dt>
<dd>Additional padding applied to output</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCW' - order
2) 'NWC' - order
- N: Number of elements in batches
- C: Number of channels
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transposed_convolution1d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 1d-transposed-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        output_padding (int): Additional padding applied to output
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCW&#39; - order
                        2) &#39;NWC&#39; - order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;transposed_convolution1d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution2d"><code class="name flex">
<span>def <span class="ident">transposed_convolution2d</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', output_padding=0, groups=1, dilation=1, use_bias=True, layout='NCHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 2d-transposed-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>output_padding</code></strong> :&ensp;<code>int</code></dt>
<dd>Additional padding applied to output</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCHW' - Order
2) 'NHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transposed_convolution2d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 2d-transposed-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        output_padding (int): Additional padding applied to output
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCHW&#39; - Order
                        2) &#39;NHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;transposed_convolution2d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
<dt id="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution3d"><code class="name flex">
<span>def <span class="ident">transposed_convolution3d</span></span>(<span>self, output_channels=3, kernel_size=3, stride=1, padding='in_eq_out', output_padding=0, groups=1, dilation=1, use_bias=True, layout='NCDHW', uid=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Append 3d-transposed-convolution to custom network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features for this layer</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel matrix size </dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, <code>tuple</code></dt>
<dd>kernel movement stride
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code>, <code>tuple</code>, <code>str</code></dt>
<dd>Zero padding applied to input
1) "in_eq_out": Automated padding applied to keep output shape same as input
2) integer or tuple value: Manually add padding </dd>
<dt><strong><code>output_padding</code></strong> :&ensp;<code>int</code></dt>
<dd>Additional padding applied to output</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of groups for grouped convolution</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code></dt>
<dd>Factor for dilated convolution</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, learnable bias is added</dd>
<dt><strong><code>layout</code></strong> :&ensp;<code>str</code></dt>
<dd>Either of these values (order)
1) 'NCDHW' - Order
2) 'NDHWC' - Order
- N: Number of elements in batches
- C: Number of channels
- D: Depth of features in layers
- H: Height of features in layers
- W: Number of features in layers</dd>
<dt><strong><code>uid</code></strong> :&ensp;<code>str</code></dt>
<dd>Unique name for layer, if not mentioned then dynamically assigned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>Containing all the parameters set as per function arguments</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transposed_convolution3d(self, output_channels=3, kernel_size=3, stride=1, padding=&#34;in_eq_out&#34;, 
    output_padding=0, groups=1, dilation=1, use_bias=True, layout=&#39;NCDHW&#39;, uid=None):
    &#39;&#39;&#39;
    Append 3d-transposed-convolution to custom network

    Args:
        output_channels (int): Number of output features for this layer
        kernel_size (int, tuple): kernel matrix size 
        stride (int, tuple): kernel movement stride  
        padding (int, tuple, str): Zero padding applied to input
                                    1) &#34;in_eq_out&#34;: Automated padding applied to keep output shape same as input
                                    2) integer or tuple value: Manually add padding 
        output_padding (int): Additional padding applied to output
        groups (int): Number of groups for grouped convolution
        dilation (int): Factor for dilated convolution
        use_bias (bool): If True, learnable bias is added
        layout (str): Either of these values (order)
                        1) &#39;NCDHW&#39; - Order
                        2) &#39;NDHWC&#39; - Order
                        - N: Number of elements in batches
                        - C: Number of channels
                        - D: Depth of features in layers
                        - H: Height of features in layers
                        - W: Number of features in layers
        uid (str): Unique name for layer, if not mentioned then dynamically assigned

    Returns:
        dict: Containing all the parameters set as per function arguments
    &#39;&#39;&#39;
    tmp={};
    tmp[&#34;uid&#34;] = uid;
    tmp[&#34;name&#34;] = &#34;transposed_convolution3d&#34;;
    tmp[&#34;params&#34;] = {};
    tmp[&#34;params&#34;][&#34;output_channels&#34;] = output_channels;
    tmp[&#34;params&#34;][&#34;kernel_size&#34;] = kernel_size;
    tmp[&#34;params&#34;][&#34;stride&#34;] = stride;
    tmp[&#34;params&#34;][&#34;padding&#34;] = padding;
    tmp[&#34;params&#34;][&#34;output_padding&#34;] = output_padding;
    tmp[&#34;params&#34;][&#34;groups&#34;] = groups;
    tmp[&#34;params&#34;][&#34;dilation&#34;] = dilation;
    tmp[&#34;params&#34;][&#34;use_bias&#34;] = use_bias;
    tmp[&#34;params&#34;][&#34;layout&#34;] = layout;
    return tmp;</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="monk.gluon.finetune" href="index.html">monk.gluon.finetune</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers">prototype_layers</a></code></h4>
<ul class="">
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.add" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.add">add</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_dropout" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_dropout">append_dropout</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_elu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_elu">append_elu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_leakyrelu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_leakyrelu">append_leakyrelu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_linear" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_linear">append_linear</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_prelu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_prelu">append_prelu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_relu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_relu">append_relu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_selu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_selu">append_selu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_sigmoid" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_sigmoid">append_sigmoid</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_softplus" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_softplus">append_softplus</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_softsign" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_softsign">append_softsign</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_swish" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_swish">append_swish</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.append_tanh" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.append_tanh">append_tanh</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling">average_pooling</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling1d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling1d">average_pooling1d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling2d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling2d">average_pooling2d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling3d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.average_pooling3d">average_pooling3d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.batch_normalization" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.batch_normalization">batch_normalization</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.concatenate" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.concatenate">concatenate</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.conv_bn_relu_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.conv_bn_relu_block">conv_bn_relu_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution">convolution</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution1d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution1d">convolution1d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution2d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution2d">convolution2d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution3d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.convolution3d">convolution3d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.densenet_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.densenet_block">densenet_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.dropout" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.dropout">dropout</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.elu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.elu">elu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.flatten" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.flatten">flatten</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.fully_connected" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.fully_connected">fully_connected</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.gelu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.gelu">gelu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling">global_average_pooling</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling1d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling1d">global_average_pooling1d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling2d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling2d">global_average_pooling2d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling3d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_average_pooling3d">global_average_pooling3d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling">global_max_pooling</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling1d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling1d">global_max_pooling1d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling2d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling2d">global_max_pooling2d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling3d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.global_max_pooling3d">global_max_pooling3d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.identity" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.identity">identity</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_a_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_a_block">inception_a_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_b_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_b_block">inception_b_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_c_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_c_block">inception_c_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_d_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_d_block">inception_d_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_e_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.inception_e_block">inception_e_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.instance_normalization" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.instance_normalization">instance_normalization</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.layer_normalization" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.layer_normalization">layer_normalization</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.leaky_relu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.leaky_relu">leaky_relu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling">max_pooling</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling1d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling1d">max_pooling1d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling2d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling2d">max_pooling2d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling3d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.max_pooling3d">max_pooling3d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.mobilenet_v2_inverted_linear_bottleneck_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.mobilenet_v2_inverted_linear_bottleneck_block">mobilenet_v2_inverted_linear_bottleneck_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.mobilenet_v2_linear_bottleneck_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.mobilenet_v2_linear_bottleneck_block">mobilenet_v2_linear_bottleneck_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.prelu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.prelu">prelu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.relu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.relu">relu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v1_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v1_block">resnet_v1_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v1_bottleneck_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v1_bottleneck_block">resnet_v1_bottleneck_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v2_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v2_block">resnet_v2_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v2_bottleneck_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.resnet_v2_bottleneck_block">resnet_v2_bottleneck_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.resnext_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.resnext_block">resnext_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.selu" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.selu">selu</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.separable_convolution_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.separable_convolution_block">separable_convolution_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.sigmoid" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.sigmoid">sigmoid</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.softplus" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.softplus">softplus</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.softsign" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.softsign">softsign</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.squeezenet_fire_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.squeezenet_fire_block">squeezenet_fire_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.subbranch_block" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.subbranch_block">subbranch_block</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.swish" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.swish">swish</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.tanh" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.tanh">tanh</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution">transposed_convolution</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution1d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution1d">transposed_convolution1d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution2d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution2d">transposed_convolution2d</a></code></li>
<li><code><a title="monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution3d" href="#monk.gluon.finetune.level_8_layers_main.prototype_layers.transposed_convolution3d">transposed_convolution3d</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>